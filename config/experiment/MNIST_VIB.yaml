# @package _global_
defaults:
  - /model: VIB
  - /data: MNIST
  - /optimization: batch_ADAM

# Definitions of the architectures used by the VAE model
architectures:
  prior:
    _target_: code.architectures.base.DiagonalNormal
    z_dim: ${params.z_dim}
  predictor:
    _target_: code.architectures.MNIST.LatentClassifier
    layers: ${params.classifier_layers}
    z_dim: ${params.z_dim}
  encoder:
    _target_: code.architectures.MNIST.Encoder
    layers: ${params.encoder_layers}
    z_dim: ${params.z_dim}

# Evaluation callbacks
callbacks:
  - _target_: code.callbacks.EvaluationCallback
    name: Accuracy/Train
    evaluate_every: 10 seconds
    evaluator:
      _target_: code.evaluation.accuracy.AccuracyEvaluation
      evaluate_on: train
      n_samples: 2048
  - _target_: code.callbacks.EvaluationCallback
    name: Accuracy/Validation
    evaluate_every: 10 seconds
    evaluator:
      _target_: code.evaluation.accuracy.AccuracyEvaluation
      evaluate_on: valid
      n_samples: 2048
  - _target_: code.callbacks.EvaluationCallback
    name: ELBO/Validation
    evaluate_every: 10 seconds
    evaluator:
      _target_: code.evaluation.elbo.ELBOEvaluation
      evaluate_on: valid
      n_samples: 2048
  - _target_: code.callbacks.EvaluationCallback
    name: ELBO/Train
    evaluate_every: 10 seconds
    evaluator:
      _target_: code.evaluation.elbo.ELBOEvaluation
      evaluate_on: train
      n_samples: 2048
  - _target_: code.callbacks.LossItemsLogCallback
    log_every: 10 seconds

# hyper-parameters for the architectures
params:
  z_dim: 64
  beta: 0.5
  lr: 1e-3
  batch_size: 128
  encoder_layers: [ 1024, 128 ]
  classifier_layers: [64, 32]

# Seed for the model
seed: 42

